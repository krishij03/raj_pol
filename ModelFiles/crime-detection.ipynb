{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":170620,"sourceType":"datasetVersion","datasetId":75548}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tensorflow.keras.utils import to_categorical\nimport itertools\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function for Feature Extraction\ndef feature_extraction(video_path):\n    width=60\n    height=60\n    sequence_length=5\n    frames_list=[]\n    #Read the Video\n    video_reader = cv2.VideoCapture(video_path)\n    #get the frame count\n    frame_count=int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    #Calculate the interval after which frames will be added to the list\n    skip_interval = max(int(frame_count/sequence_length), 1)\n    #iterate through video frames\n    for counter in range(sequence_length):\n        #Set the current frame postion of the video\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, counter * skip_interval)\n        #Read the current frame \n        ret, frame = video_reader.read()\n        if not ret:\n            break;\n        #Resize the image\n        frame=cv2.resize(frame, (height, width))\n        frame = frame/255\n        #Append to the frame\n        frames_list.append(frame)\n    video_reader.release()\n    #Return the Frames List\n    return frames_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_video(datasets, label):\n#     global image\n    labels=[]\n    images=[]\n    #Iterate through each foler corresponding to category\n    for folder in datasets:\n        print(folder)\n        for file in tqdm(os.listdir(folder)):\n            #Get the path name for each video\n            video_path = os.path.join(folder, file)\n            #Extract the frames of the current video\n            if file.endswith(\".mp4\") or file.endswith(\".avi\"):\n                frames_list = feature_extraction(video_path)\n                images.append(frames_list)\n                labels.append(label)\n            else:\n                print(file)\n                raise Exception(\"so shit\")\n    return np.array(images, dtype='float16'), np.array(labels, dtype='int8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_normal_video(datasets, label):\n#     global image\n    labels=[]\n    images=[]\n    #Iterate through each foler corresponding to category\n    for file in tqdm(os.listdir(datasets)):\n        #Get the path name for each video\n        video_path = os.path.join(datasets, file)\n        #Extract the frames of the current video\n        frames_list = feature_extraction(video_path)\n        images.append(frames_list)\n        labels.append(label)\n    return np.array(images, dtype='float16'), np.array(labels, dtype='int8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_path = '../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/'\nimages0, labels0 = load_normal_video(normal_path, 0) #0 for normal","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abnomaly_path = ['../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Abuse',\n                 '../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest',\n                 '../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arson',\n                 '../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Assault',\n               '../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Burglary',\n                '../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Explosion',\n                '../input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Fighting'] \nimages, labels = load_video(abnomaly_path, 1) #1 for abnomaly","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_images = np.concatenate((images, images0), axis=0)\nall_labels = np.concatenate((labels, labels0), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test=train_test_split(all_images, all_labels, test_size=0.06, random_state=10)\nx_train.shape, x_test.shape, np.array(y_train).shape, np.array(y_test).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2, input_shape=(x_train.shape[1],x_train.shape[2], x_train.shape[3], 3)))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(Flatten())\n          \nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model training\nes = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\nhistory = model.fit(x_train, to_categorical(y_train), batch_size=31, epochs=20, validation_data=(x_test, to_categorical(y_test)), callbacks=[es])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_weights()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the graph to check training and testing accuracy over the period of time\nplt.figure(figsize=(13,5))\nplt.title(\"Accuracy vs Epochs\")\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Test Accuracy')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)\npredicted_classes=[]\nfor i in range(len(y_test)):\n    predicted_classes.append(np.argmax(y_pred[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, predicted_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion Matrix\nplt.figure(figsize=(25,25))\nplt.title(\"Confusion matrix\")\ncm=confusion_matrix(y_test, predicted_classes)\nplt.imshow(cm)\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, cm[i, j], horizontalalignment=\"center\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('CNN_Model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_weights = model.get_weights()\n\nlayer_names = [layer.name for layer in model.layers]\n\nfor layer_name, weights in zip(layer_names, final_weights):\n    print(f\"Layer: {layer_name}, Weights Shape: {weights.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}